{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classifying the ratings of the IMDb movie's reviews**. Used **Hierarchical attention** as explained in [Hierarchical Attention Networks for Document Classification](https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0,1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from  matplotlib import  pyplot as  plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_SIZE = 50\n",
    "OUT_SIZE = 10  # 1-10  (0's excluded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = Path('./aclImdb/')\n",
    "TRAIN_PATH = Path('./aclImdb/train/')\n",
    "TEST_PATH = Path('./aclImdb/test/')\n",
    "\n",
    "pos_dir = 'pos'\n",
    "neg_dir = 'neg' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Create DataFrame | x | y | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_df(data_path:Path):\n",
    "    '''\n",
    "    \n",
    "    :param data_path: either path to training folder or to testing folder.\n",
    "    '''\n",
    "    file_paths = []\n",
    "    ratings = []\n",
    "    for sent_type in [pos_dir,neg_dir]:\n",
    "        for file in (data_path/sent_type).iterdir():\n",
    "            file_paths.append(file)\n",
    "            ratings.append(int(file.parts[-1].split('_')[1].split('.')[0]))\n",
    "\n",
    "    return pd.DataFrame(list(zip(file_paths,ratings)), columns=['file_paths', 'ratings'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80%-20% for train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df = construct_df(TRAIN_PATH)\n",
    "\n",
    "train_idx = np.random.rand(len(data_df)) < .8\n",
    "\n",
    "train_df =  data_df[train_idx].reset_index(drop=True)\n",
    "val_df = data_df[~train_idx].reset_index(drop=True)\n",
    "test_df = construct_df(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     5100\n",
       "10    4732\n",
       "8     3009\n",
       "4     2696\n",
       "7     2496\n",
       "3     2420\n",
       "2     2284\n",
       "9     2263\n",
       "Name: ratings, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.ratings.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_paths</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aclImdb/train/pos/5826_10.txt</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aclImdb/train/pos/2621_8.txt</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aclImdb/train/pos/9528_9.txt</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aclImdb/train/pos/1890_10.txt</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aclImdb/train/pos/9858_7.txt</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      file_paths  ratings\n",
       "0  aclImdb/train/pos/5826_10.txt       10\n",
       "1   aclImdb/train/pos/2621_8.txt        8\n",
       "2   aclImdb/train/pos/9528_9.txt        9\n",
       "3  aclImdb/train/pos/1890_10.txt       10\n",
       "4   aclImdb/train/pos/9858_7.txt        7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I found this film to be the usual French slap in America's face. The camera, all too often, focuses on fat people, on sloppy homes and on tacky rural areas. While the narration seems to sympathize with and admire the small town folks who are introduced to the viewer, the cinematography exploits and demeans them. There were, undoubtedly, thin people to be seen in Glencoe and neat, organized homes, but Malle chose to show us the worst of what was there to be seen. <br /><br />I can only hope that some American filmmakers will go to France to reveal to the American public its worst elements. I can assure you, as a frequent visitor to France, that all is not well there. Foreign immigrants are not readily assimilated, thus creating severe social inequities. But Americans are not eager to unmask the French for their prejudice toward their own compatriots and their envy toward the U.S., so we're not likely to see films on the subject.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in  doc\n",
    "open(train_df.iloc[15,0], 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re_br = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)\n",
    "def sub_br(x): return re_br.sub(\"\\n\", x.lower())\n",
    "\n",
    "def split_sentece(x, splt_char=['.','!','?']): \n",
    "    ''' Splits text over the different tokens'''\n",
    "    A = x if isinstance(x, list) else [x]\n",
    "    B = A\n",
    "    for char in splt_char:\n",
    "        A = B\n",
    "        B = []\n",
    "        for sentence in A:\n",
    "            if len(sentence)>0:\n",
    "                B.extend(sentence.split(char))\n",
    "    return B\n",
    "\n",
    "my_tok = spacy.load('en')\n",
    "def spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(sub_br(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this ● is ● one ● of ● those ● movies ● that ● i ● 've ● seen ● so ● many ● times ● that ● i ● can ● quote ● most ● of ● it\n",
      "  ● some ● of ● the ● lines ● in ● this ● movie ● are ● just ● unbeatable\n",
      "  ● i ● particularly ● enjoy ● watching ● him ● stumble ● and ● fall ● while ● drunk ● , ● go ● out ● to ● the ● fancy ● restaurant ● drunk ● and ● the ● part ● with ● the ● moose\n",
      "\n",
      "\n",
      " ● i ● do ● n't ● know ● how ● many ● times ● i ● have ● seen ● this ● sequence ● but ● it ● 's ● funny ● every ● time\n",
      "  ● from ● the ● moment ● arthur ● gets ● to ● susan ● 's ● dad ● 's ● place ● to ● the ● bit ● with ● the ● moose ● , ● you ● pretty ● much ● laugh ● the ● whole ● time\n",
      "  ● i ● remember ● watching ● the ● out ● - ● takes ● regarding ● the ● bit ● with ● the ● moose\n",
      "  ● it ● went ● down ● just ● like ● i ● 'd ● imagined ● it ● 'd ● be ● like\n",
      "  ● they ● were ● all ● laughing ● so ● hard ● it ● was ● difficult ● for ● them ● to ● film ● it\n",
      "\n",
      "\n",
      " ● the ● late ● sir ● john ● gielgud ● was ● a ● wonderful ● addition ● to ● this\n",
      "  ● his ● demeanor ● , ● his ● one ● - ● liners ● and ● the ● way ● he ● handled ● arthur ● were ● all ● equally ● hilarious\n",
      "  ● it ● 's ● always ● a ● funny ● moment ● when ● he ● whacks ● him ● over ● the ● head ● with ● his ● hat ● or ● tells ● him ● he ● 's ● a ● spoiled ● little ● _ ● _ ● _ ● _\n",
      "  ● i ● laugh ● every ● time ● i ● listen ● to ● the ● \" ● i ● 'm ● going ● to ● have ● a ● bath ● \" ● and ● the ● lines ● that ● follow\n"
     ]
    }
   ],
   "source": [
    "file = train_df.iloc[3,0]\n",
    "file_sentences_level = split_sentece(sub_br(file.read_text()))\n",
    "file_word_level = [spacy_tok(sentence) for sentence in file_sentences_level]\n",
    "print('\\n'.join([' ● '.join(sentence) for sentence in file_word_level]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose to use all the vocabulary in training and validation to avoid re-computations in testing time afterwards. (cheater! I know) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "for path in train_df.file_paths:\n",
    "    counts.update(spacy_tok(path.read_text(encoding='utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out words that appear less than 10 times because they are likely to don't appear in the test set. We will set them to unknown `<UNK>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab2index = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "words = [\"<PAD>\", \"<UNK>\"]\n",
    "for word in counts:\n",
    "    if counts[word]>10:\n",
    "        vocab2index[word] = len(words)\n",
    "        words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words\n",
    "list(vocab2index.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We contemplate {len(words)} words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order docs by length (in  terms of sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_length(file_path):\n",
    "    return len(split_sentece(sub_br(file_path.read_text('utf-8'))))\n",
    "\n",
    "def order_docs(df):\n",
    "    order = []\n",
    "    for i, path in enumerate(df.file_paths):\n",
    "        order.append((i, doc_length(path)))\n",
    "    \n",
    "    idxs = [x[0] for x in sorted(order, key=lambda x: x[1])]\n",
    "    \n",
    "    return df.iloc[idxs,:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_df = order_docs(train_df)\n",
    "# val_df = order_docs(val_df)\n",
    "# test_df = order_docs(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.ratings.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs = 64\n",
    "MAX_SENT = 148\n",
    "MAX_WORDS = 2802"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class doc_s_w(Dataset):\n",
    "    \n",
    "    def __init__(self, df, def_idx=1):\n",
    "        \n",
    "        df = order_docs(df)\n",
    "        self.paths = df.file_paths\n",
    "        self.y = df.ratings\n",
    "        self.def_idx = def_idx\n",
    "        \n",
    "    def __len__(self,):\n",
    "        return len(self.y)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        file = self.paths[idx]\n",
    "        doc_sentences_level = split_sentece(sub_br(file.read_text('utf-8')))\n",
    "        doc_word_level = [spacy_tok(sentence) for sentence in doc_sentences_level]\n",
    "        x = [[vocab2index.get(w, self.def_idx) for w in s] for s in doc_word_level]\n",
    "        \n",
    "        max_n_words = max([len(s) for s in x])\n",
    "        \n",
    "        return x, self.y[idx]-1, len(x), max_n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dynamic_word_sentece_padding(batch, MAX_SENT=MAX_SENT, MAX_WORDS=MAX_WORDS):\n",
    "    '''\n",
    "    We have set a maximum number of words and sentences. \n",
    "    If a doc goes over we just ignore the tail.\n",
    "    ''' \n",
    "    \n",
    "#     compute dimensions of batch\n",
    "    dim_sent = min(MAX_SENT, max([b[2] for b in batch]))\n",
    "    dim_words = min(MAX_WORDS, max([b[3] for b in batch]))\n",
    "    \n",
    "#     Create sentece input\n",
    "    X =[]\n",
    "    for sentences,*_ in batch:\n",
    "        A = np.zeros([dim_sent, dim_words])\n",
    "        for i in range(min([len(sentences),dim_sent])):\n",
    "            fill_up_to = min(len(sentences[i]), dim_words)\n",
    "            A[i,:fill_up_to] = sentences[i][:fill_up_to]\n",
    "        X.append(A)\n",
    "            \n",
    "    y = [b[1] for b in batch]\n",
    "\n",
    "    new_batches = list(zip(X,y))\n",
    "    return default_collate(new_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "# train_ds = doc_s_w(train_df)\n",
    "# val_ds = doc_s_w(val_df)\n",
    "# test_ds = doc_s_w(test_df)\n",
    "\n",
    "# train_dl = DataLoader(train_ds, shuffle=False, batch_size=64, collate_fn=dynamic_word_sentece_padding, drop_last=True)\n",
    "# val_dl = DataLoader(val_ds, shuffle=False, batch_size=64, collate_fn=dynamic_word_sentece_padding, drop_last=True)\n",
    "# test_dl = DataLoader(test_ds, shuffle=False, batch_size=64, collate_fn=dynamic_word_sentece_padding, drop_last=True)\n",
    "\n",
    "\n",
    "# x, y = next(iter(train_dl))\n",
    "# x, y  = x.long(), y.long()\n",
    "# x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self,  input_size, dropout=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_size, input_size)\n",
    "                \n",
    "        # To be tested:\n",
    "        self.linear2 = nn.Linear(input_size, 1, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = F.tanh(self.linear1(x))\n",
    "        \n",
    "        out = self.linear2(out)\n",
    "\n",
    "        out = out.squeeze()\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "\n",
    "        w = F.softmax(out, dim=-1)\n",
    "            \n",
    "#         w = self.dropout(w)#/torch.sum(w.unsqueeze(-1),-1)\n",
    "        \n",
    "        out = torch.bmm(w.unsqueeze(-1).permute(0,2,1), x).squeeze()\n",
    "\n",
    "        return out\n",
    "    \n",
    "class HiererchicalAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_size, vocab_size, bs, out_size,\n",
    "                dropout=0, max_words=MAX_WORDS, max_sentences=MAX_SENT):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.bs = bs\n",
    "\n",
    "        # word embbeding\n",
    "        self.emb = nn.Embedding(num_embeddings=vocab_size, \n",
    "                                embedding_dim=embedding_dim)\n",
    "        \n",
    "        # word level\n",
    "        self.word_GRU = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size,\n",
    "                               bidirectional=True, batch_first=True, dropout=dropout)\n",
    "    \n",
    "        self.word_Attention = Attention(2*hidden_size, dropout=dropout)\n",
    "        \n",
    "        # sentece level\n",
    "        self.sentence_GRU = nn.GRU(input_size=2*hidden_size, hidden_size=hidden_size,\n",
    "                               bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.sentence_Attention = Attention(2*hidden_size, dropout=dropout)\n",
    "        \n",
    "        # final linear\n",
    "        self.linear = nn.Linear(2*hidden_size, out_size)\n",
    "        \n",
    "    def forward(self, x, h_0):\n",
    "        \n",
    "        sent_encoding  = []\n",
    "        \n",
    "        # GRU + Attention (word - level) - :output: vector encoding the sentence \n",
    "        for sent_idx in range(x.shape[1]):\n",
    "            \n",
    "            x_i = x[:,sent_idx,:]\n",
    "            \n",
    "            out_i = self.emb(x_i)\n",
    "            \n",
    "            out_i = self.word_GRU(out_i, h_0)[0]\n",
    "            \n",
    "            out_i = self.word_Attention(out_i)\n",
    "\n",
    "            sent_encoding.append(out_i.unsqueeze(-2))\n",
    "        \n",
    "        out = torch.cat(sent_encoding, dim=-2)\n",
    "        \n",
    "        # GRU + Attention (sentence - level) - :output: vector encoding the sentence \n",
    "        # We don't want sentence level attention if there is only one sentence (this just adds noise)\n",
    "        if x.shape[1]>1:\n",
    "            \n",
    "            out = self.sentence_GRU(out, h_0)[0]\n",
    "\n",
    "            out = self.sentence_Attention(out)\n",
    "        \n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out.squeeze()\n",
    "    \n",
    "    def initHidden(self, cuda=True):\n",
    "        hidden=torch.zeros(2, self.bs, self.hidden_size)\n",
    "        return hidden.cuda() if cuda else hidden\n",
    "\n",
    "\n",
    "# class Hiererchical_attention(nn.Module):\n",
    "    \n",
    "#     def __init__(self, embedding_dim, hidden_size, vocab_size, bs, out_size,\n",
    "#                 dropout=0, max_words=MAX_WORDS, max_sentences=MAX_SENT):\n",
    "        \n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.bs = bs\n",
    "\n",
    "#         # word embbeding\n",
    "#         self.emb = nn.Embedding(num_embeddings=vocab_size, \n",
    "#                                 embedding_dim=embedding_dim)\n",
    "        \n",
    "#         # word level\n",
    "#         self.word_GRU = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size,\n",
    "#                                bidirectional=True, batch_first=True, dropout=dropout)\n",
    "\n",
    "#         self.word_Attention = Attention(2*hidden_size, dropout=dropout)\n",
    "        \n",
    "#         # sentece level\n",
    "#         self.sentence_GRU = nn.GRU(input_size=2*hidden_size, hidden_size=hidden_size,\n",
    "#                                bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        \n",
    "# #         self.posit_enc_s = positional_encoding(max_sentences, 2*hidden_size)\n",
    "        \n",
    "#         self.sentence_Attention = Attention(2*hidden_size, dropout=dropout)\n",
    "        \n",
    "#         # final linear\n",
    "#         self.linear = nn.Linear(2*hidden_size, out_size)\n",
    "        \n",
    "#     def forward(self, x, h_0):\n",
    "        \n",
    "#         sent_encoding  = []\n",
    "        \n",
    "#         # Same GRU + Attention (word) - :output: vector encoding the sentence \n",
    "#         for sent_idx in range(x.shape[1]):\n",
    "            \n",
    "#             x_i = x[:,sent_idx,:]\n",
    "            \n",
    "#             out_i = self.emb(x_i)\n",
    "                        \n",
    "#             out_i = self.word_GRU(out_i, h_0)[0]\n",
    "            \n",
    "#             #pos encoder\n",
    "            \n",
    "            \n",
    "#             out_i = self.word_Attention(out_i)\n",
    "\n",
    "#             sent_encoding.append(out_i.unsqueeze(-2))\n",
    "        \n",
    "#         out = torch.cat(sent_encoding, dim=-2)\n",
    "        \n",
    "#         # Same GRU + Attention (sentence) - :output: vector encoding the sentence \n",
    "#         # We don't want sentence level attention if there is only one sentence\n",
    "#         if x.shape[1]>1:\n",
    "            \n",
    "#             out = self.sentence_GRU(out, h_0)[0]\n",
    "\n",
    "#             out = self.sentence_Attention(out)\n",
    "        \n",
    "#         out = self.linear(out)\n",
    "        \n",
    "#         return out.squeeze()\n",
    "    \n",
    "#     def initHidden(self, cuda=True):\n",
    "#         hidden=torch.zeros(2, self.bs, self.hidden_size)\n",
    "#         return hidden.cuda() if cuda else hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Testing\n",
    "# net = HiererchicalAttention(embedding_dim=EMBEDDING_DIM,\n",
    "#                              hidden_size=HIDDEN_SIZE, \n",
    "#                              vocab_size=vocab_size, \n",
    "#                              bs=BATCH_SIZE, out_size=OUT_SIZE).cuda().cuda()\n",
    "# h_0 = net.initHidden()\n",
    "# x.shape, net(x.long().cuda(), h_0).shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "\n",
    "\n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cos_cycle(start_lr, end_lr, n_iterations):\n",
    "    '''cosine annealing'''\n",
    "    i = np.arange(n_iterations)\n",
    "    c_i = 1 + np.cos(i*np.pi/n_iterations)\n",
    "    return end_lr + (start_lr - end_lr)/2 *c_i\n",
    "    \n",
    "    \n",
    "class step_policy:\n",
    "    '''\n",
    "    One-cycle learning rate and momentum policy with cosine annealing.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_epochs, dl, max_lr, div_factor:float=25., pctg:float=.3, moms:tuple=(.95,.85), delta=1/1e4):\n",
    "        \n",
    "        total_iterations = n_epochs*len(dl)\n",
    "        \n",
    "        max_lr, min_start, min_end = (max_lr, \n",
    "                                      max_lr/div_factor, \n",
    "                                      max_lr/div_factor*delta)\n",
    "        \n",
    "        self.stages = (int(total_iterations*pctg), total_iterations - int(total_iterations*pctg))\n",
    "        \n",
    "        lr_diffs = ((min_start, max_lr),(max_lr, min_end))\n",
    "        mom_diffs = (moms, (moms[1],moms[0]))\n",
    "\n",
    "        self.lr_schedule = self._create_schedule(lr_diffs)\n",
    "        self.mom_schedule = self._create_schedule(mom_diffs)\n",
    "        \n",
    "        self.iter = -1\n",
    "        \n",
    "    def _create_schedule(self, diffs):\n",
    "        individual_stages = [cos_cycle(start, end, n) for ((start, end),n) in zip(diffs, self.stages)]\n",
    "        return np.concatenate(individual_stages)\n",
    "    \n",
    "    def step(self):\n",
    "        self.iter += 1\n",
    "        return [sch[self.iter] for sch in [self.lr_schedule, self.mom_schedule]]\n",
    "    \n",
    "    \n",
    "    \n",
    "class OptimizerWrapper:\n",
    "    '''\n",
    "    Wrapper to use wight decay in optim.Adam without influencing its algorithm.\n",
    "    Takes care of the change in learning rate / momenutm at every iteration.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, model, n_epochs, dl, max_lr, div_factor=None, wd=0):\n",
    "        \n",
    "        self.policy =  step_policy(n_epochs=n_epochs, dl=dl, \n",
    "                                   max_lr=max_lr, div_factor=div_factor)\n",
    "        \n",
    "        self.model = model\n",
    "        self._wd = wd\n",
    "        \n",
    "        p = filter(lambda x: x.requires_grad, model.parameters())\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=p, lr=0)\n",
    "    \n",
    "    def _update_optimizer(self):\n",
    "        lr_i, mom_i = self.policy.step()\n",
    "        for group in self.optimizer.param_groups:\n",
    "            group['lr'] = lr_i\n",
    "            group['betas'] = (mom_i, .999)\n",
    "\n",
    "    def step(self):\n",
    "        self._update_optimizer()\n",
    "        if self._wd!=0:\n",
    "            for group in self.optimizer.param_group:\n",
    "                for p  in group['params']: p.data.mul_(group['lr']*self._wd)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def zero_grad(self): self.optimizer.zero_grad()\n",
    "        \n",
    "    def reset(self, n_epochs, dl, max_lr):\n",
    "        self.iter = -1\n",
    "        self.policy =  step_policy(n_epochs=n_epochs, dl=dl, max_lr=max_lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    m = x.max(1)\n",
    "    num = np.exp(x-np.expand_dims(m,1))\n",
    "    den = np.exp(x-np.expand_dims(m,1)).sum(1)\n",
    "    return num/np.expand_dims(den,1)\n",
    "\n",
    "def accuracy(y, pred):\n",
    "    \"\"\"\n",
    "    Average Accuracy score\n",
    "    \"\"\"\n",
    "    pred = pred.argmax(-1)\n",
    "    return accuracy_score(y, pred)\n",
    "\n",
    "\n",
    "def validate(model, valid_dl, h_0):\n",
    "    \"\"\"Validation/Testing loop\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    div = 0\n",
    "    agg_loss = 0\n",
    "    ys = np.empty((0), int)\n",
    "    preds = np.empty((0, 10), float)\n",
    "    for it, (x,y) in enumerate(valid_dl):\n",
    "        \n",
    "        x = x.long().cuda()\n",
    "        y = y.long().cuda()\n",
    "        \n",
    "        out = model(x, h_0)\n",
    "        loss = F.cross_entropy(input=out,target=y)\n",
    "\n",
    "        agg_loss += loss.item()\n",
    "        div += 1\n",
    "        \n",
    "        preds = np.append(preds, out.cpu().detach().numpy(), axis=0)\n",
    "        ys = np.append(ys, y.cpu().numpy(), axis=0)\n",
    "    \n",
    "    preds = softmax(preds)\n",
    "    val_loss = agg_loss/div\n",
    "    measures = accuracy(ys, preds)\n",
    "    model.train()\n",
    "    return val_loss, measures\n",
    "      \n",
    "\n",
    "def train(n_epochs, train_dl, model, h_0, valid_dl=None, max_lr=.01, div_factor=25):\n",
    "    \"\"\"Training loop\n",
    "    \"\"\"\n",
    "            \n",
    "    optimizer = OptimizerWrapper(model, n_epochs, train_dl,\n",
    "                                 max_lr=max_lr, div_factor=div_factor)\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(n_epochs)):\n",
    "        model.train()\n",
    "        div = 0\n",
    "        agg_loss = 0\n",
    "        for it, (x,y) in enumerate(train_dl):\n",
    "            \n",
    "            x = x.long().cuda()\n",
    "            y = y.long().cuda()\n",
    "            \n",
    "            out = model(x, h_0)\n",
    "            loss = F.cross_entropy(input=out,target=y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            agg_loss += loss.item()\n",
    "            div += 1\n",
    "            \n",
    "        if valid_dl is None: print(f'Ep. {epoch+1} - train loss {agg_loss/div:.4f}')\n",
    "        else:\n",
    "            val_loss, measure = validate(model, valid_dl, h_0)\n",
    "            print(f'Ep. {epoch+1} - train loss {agg_loss/div:.4f} -  val loss {val_loss:.4f} AUC {measure:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify the AUC function to compute the average AUC for each class leaving out those corresponding to ranks 5 and 6 because we don't have observations of those rankings. When only one type of observations of the true label exist the AUC is not defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = doc_s_w(train_df)\n",
    "val_ds = doc_s_w(val_df)\n",
    "test_ds = doc_s_w(test_df)\n",
    "\n",
    "train_dl = DataLoader(train_ds, shuffle=True, batch_size=BATCH_SIZE, collate_fn=dynamic_word_sentece_padding, drop_last=True)\n",
    "val_dl = DataLoader(val_ds, shuffle=False, batch_size=BATCH_SIZE, collate_fn=dynamic_word_sentece_padding, drop_last=True)\n",
    "test_dl = DataLoader(test_ds, shuffle=False, batch_size=BATCH_SIZE, collate_fn=dynamic_word_sentece_padding, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = HiererchicalAttention(embedding_dim=EMBEDDING_DIM,\n",
    "                             hidden_size=HIDDEN_SIZE, \n",
    "                             vocab_size=vocab_size, \n",
    "                             bs=BATCH_SIZE, out_size=OUT_SIZE, dropout=.3).cuda()\n",
    "\n",
    "h_0 = model.initHidden(cuda=True)\n",
    "\n",
    "train(n_epochs, train_dl, model, h_0, val_dl, max_lr=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train(4, train_dl, model, h_0, val_dl, div_factor=1e2, max_lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_model(model, './firs_model.pth')\n",
    "torch.save(h_0, 'firs_model_hidden_state.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_loss, measure = validate(model, test_dl, h_0)\n",
    "print(f'testing loss {val_loss:.4f} AUC {measure:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
